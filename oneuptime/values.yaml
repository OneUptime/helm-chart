global:
  storageClass: 
  clusterDomain: &global-cluster-domain cluster.local


# Please change this to the domain name / IP where OneUptime server is hosted on. 
host: localhost
httpProtocol: http

# (Optional): You usually do not need to set this if you're self hosting. If you do set it, set it to a long random value.
oneuptimeSecret: 
encryptionSecret: 

# (Optional): You usually do not need to set this if you're self hosting. 
openTelemetryCollectorHost:
fluentdHost: 

deployment: 
  replicaCount: 1

metalLb: 
  enabled: false
  ipAdddressPool: 
    enabled: false
    addresses: 
      # - 51.158.55.153/32 # List of IP addresses of all the servers in the cluster.

nginx:
  service: 
    loadBalancerIP: 
    type: LoadBalancer
    externalIPs:
      # - 51.158.55.153 # Please make sure this is the same as the one in metalLb.ipAdddressPool.addresses

postgresql: 
  clusterDomain: *global-cluster-domain
  auth: 
    username: oneuptime
    database: oneuptimedb
  architecture: standalone
  primary:
    service:
      ports: 
         postgresql: "5432"
    terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5
    persistence:
      size: 25Gi
  readReplicas:
    terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5 
    persistence:
      size: 25Gi

clickhouse: 
  clusterDomain: *global-cluster-domain
  service:
    ports: 
      http: "8123"
  shards: 1
  replicaCount: 1
  terminationGracePeriodSeconds: 0 # We do this because we do not want to wait for the pod to terminate in case of node failure. https://medium.com/tailwinds-navigator/kubernetes-tip-how-statefulsets-behave-differently-than-deployments-when-node-fails-d29e36bca7d5
  zookeeper: 
    enabled: false
  persistence: 
    size: 25Gi
  auth: 
    username: oneuptime
  initdbScripts: 
    db-init.sql: |
      CREATE DATABASE oneuptime;

redis: 
  clusterDomain: *global-cluster-domain
  architecture: standalone
  auth:
    enabled: true
  master:
    service: 
      ports: 
        redis: "6379"
    persistence:
      enabled: false # We dont need redis persistence, because we dont do anything with it. 
  replica:
    persistence:
      enabled: false # We dont need redis persistence, because we dont do anything with it.
  commonConfiguration: |-
   appendonly no
   save "" 
  

image:
  registry: docker.io
  repository: oneuptime
  pullPolicy: Always
  tag: release
  restartPolicy: Always

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeEnvironment: production

billing: 
  enabled: false
  publicKey: 
  privateKey: 
  smsDefaultValueInCents: 
  callDefaultValueInCentsPerMinute:
  smsHighRiskValueInCents:
  callHighRiskValueInCentsPerMinute: 

subscriptionPlan: 
  basic: 
  growth: 
  scale: 
  enterprise: 

analytics: 
  host:
  key: 

internalSmtp:
  enabled: true
  sendingDomain: 
  dkimPrivateKey: 
  dkimPublicKey: 
  email: 
  name: 

incidents:
  disableAutomaticCreation: false

statusPage: 
  cnameRecord: 

probes: 
  one: 
    name: "Probe"
    description: "Probe"
    monitoringWorkers: 3
    monitorFetchLimit: 10
    key:
    replicaCount: 1
    syntheticMonitorScriptTimeoutInMs: 60000
    customCodeMonitorScriptTimeoutInMs: 60000
    # Feel free to leave this blank if you're not integrating this with OpenTelemetry Backend.
    openTelemetryExporter: 
      headers:
  # two: 
  #   name: "Probe 2"
  #   description: "Probe 2"
  #   monitoringWorkers: 3
  #   monitorFetchLimit: 10
  #   key:
  #   replicaCount: 1
  #   syntheticMonitorScriptTimeoutInMs: 60000
  #   customCodeMonitorScriptTimeoutInMs: 60000
  #   openTelemetryExporter:
  #     headers:

port: 
  app: 3002
  ingestor: 3400
  testServer: 3800
  accounts: 3003
  statusPage: 3105
  dashboard: 3009
  adminDashboard: 3158
  nginxHttp: 80
  nginxHttps: 443
  haraka: 2525
  probe: 3500
  otelCollectorGrpc: 4317
  otelCollectorHttp: 4318
  isolatedVM: 4572


testServer: 
  enabled: false


openTelemetryExporter:
  endpoint: 
    server:
    client:
  headers: 
    app: 
    dashboard:
    accounts:
    statusPage:
    adminDashboard:
    ingestor:
    nginx:

containerSecurityContext:
podSecurityContext:
affinity:
tolerations:
nodeSelector:


# This can be one of the following: DEBUG, INFO, WARN, ERROR
logLevel: ERROR

# Enable cleanup cron jobs
cronJobs:
  cleanup:
    enabled: false
  e2e:
    enabled: false
    isUserRegistered: false
    registeredUserEmail: 
    registeredUserPassword:
    # This is the URL of the status page you want to test. This is used to check if the status page is up and running.
    statusPageUrl:
    failedWebhookUrl: 
  

letsEncrypt: 
  # Generate a private key via openssl, encode it to base64
  accountKey: 
  # Email address to register with letsencrypt for notifications
  email: 

oneuptimeIngress: 
  enabled: false
  annotations: 
  # Please change this to the ingress class name for your cluster. If you use a cloud provider, this is usually the default ingress class name.
  # If you dont have nginx ingress controller installed, please install it by going to https://kubernetes.github.io/ingress-nginx/deploy/
  className: nginx # Required. Please change this to the ingress class name for your cluster. If you use a cloud provider, this is usually the default ingress class name.
  hosts:  # List of hosts for the ingress. Please change this to your hosts
    # - "oneuptime.com" # Host 1
    # - "www.oneuptime.com" # Host 2
  tls:
    enabled: false
    hosts: 
      # - host: "oneuptime.com" # Host 1
      #   secretName: "oneuptime-tls


script: 
  workflowScriptTimeoutInMs: 5000

# extraTemplates -- Array of extra objects to deploy with the release. Strings
# are evaluated as a template and can use template expansions and functions. All
# other objects are used as yaml.
extraTemplates:
  #- |
  #    apiVersion: v1
  #    kind: ConfigMap
  #    metadata:
  #      name: my-configmap
  #    data:
  #      key: {{ .Values.myCustomValue | quote }}